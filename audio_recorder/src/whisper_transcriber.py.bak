# whisper_transcriber.py
# Implementação específica para transcrição usando o modelo Whisper da OpenAI

import whisper
import sys
import os
import logging
import numpy as np
import torch
import time
import threading
import queue
import abc
import gc  # Garbage collector
from typing import Dict, Any, Tuple, Optional, List, Union

# Adiciona o diretório raiz ao caminho de busca para importar constants
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from constants import (
    WHISPER_MODEL, 
    DEVICE_TYPE, 
    SAMPLE_RATE,
    TEMPERATURE, 
    SEGMENT_LENGTH,
    LIMIT_HISTORY,
    MAX_HISTORY_SECONDS
)

# Definimos a classe base aqui ao invés de importá-la para evitar dependência circular
class AudioTranscriber(abc.ABC):
    """
    Classe base abstrata para transcritores de áudio.
    
    Qualquer novo transcritor deve herdar desta classe e implementar
    o método transcribe_file.
    """
    
    @abc.abstractmethod
    def transcribe_file(self, file_path: str, initial_prompt: str = None) -> str:
        """
        Transcreve um arquivo de áudio para texto.
        
        Parâmetros:
            file_path (str): Caminho para o arquivo de áudio
            initial_prompt (str, opcional): Texto inicial para dar contexto
            
        Retorna:
            str: Texto transcrito
        """
        pass

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("WhisperTranscriber")

class WhisperTranscriber(AudioTranscriber):
    """
    Transcritor de áudio usando o modelo Whisper da OpenAI.
    
    Este transcritor usa o modelo Whisper localmente para converter
    áudio em texto, otimizado para arquivos WAV em 16kHz mono.
    """
    
    def __init__(self, model_size=WHISPER_MODEL, device=DEVICE_TYPE):
        """
        Inicializa o transcritor Whisper com o modelo especificado.
        
        Parâmetros:
            model_size (str): Tamanho do modelo Whisper a carregar
                             ('tiny', 'base', 'small', 'medium', 'large')
            device (str): Dispositivo para inferência ('cpu' ou 'cuda')
        """
        super().__init__()
        logger.info(f"Inicializando WhisperTranscriber com modelo: {model_size} no dispositivo: {device}")
        
        # Define o dispositivo para CPU se CUDA não estiver disponível
        if device == "cuda" and not torch.cuda.is_available():
            logger.warning("CUDA não disponível, usando CPU")
            device = "cpu"
            
        self.device = device
        self.model_size = model_size
        
        # Carrega o modelo Whisper (pode levar algum tempo)
        logger.info(f"Carregando modelo Whisper {model_size} no dispositivo {device}...")
        start_time = time.time()
        self.model = whisper.load_model(model_size, device=device)
        logger.info(f"Modelo carregado em {time.time() - start_time:.2f} segundos")
        
        # Configurações de transcrição
        self.language = "pt"  # Idioma padrão Português
        self.translate = False  # Por padrão, não traduz para inglês
        
        # Inicializa variáveis para streaming
        self.stream_queue = None
        self.stream_thread = None
        self.streaming = False
        self.stream_callback = None
        self.stream_buffer = []
        self.accumulated_text = ""
        
        # Armazena informações sobre transcrições anteriores para melhorar a continuidade
        self._transcription_history = []
        
        # Pré-aquece o modelo com uma pequena amostra de silêncio para agilizar a primeira transcrição
        self._warmup_model()
    
    @staticmethod
    def is_cuda_available() -> bool:
        """
        Verifica se CUDA está disponível no sistema.
        
        Retorna:
            bool: True se CUDA estiver disponível, False caso contrário
        """
        return torch.cuda.is_available()
    
    def _warmup_model(self):
        """Pré-aquece o modelo com uma pequena amostra de silêncio"""
        try:
            silence = np.zeros(16000, dtype=np.float32)  # 1 segundo de silêncio a 16kHz
            start_time = time.time()
            self.model.transcribe(silence, language="pt")
            logger.info(f"Modelo pré-aquecido em {time.time() - start_time:.2f} segundos")
        except Exception as e:
            logger.warning(f"Falha ao pré-aquecer o modelo: {e}")
    
    def _normalize_audio(self, audio_array: np.ndarray, sample_rate: int = SAMPLE_RATE) -> np.ndarray:
        """
        Normaliza o áudio para melhorar a qualidade da transcrição
        
        Parâmetros:
            audio_array (np.ndarray): Array de áudio numpy
            sample_rate (int): Taxa de amostragem do áudio (16000 para Whisper)
            
        Retorna:
            np.ndarray: Array de áudio normalizado
        """
        # Verificar se é necessário fazer resampling
        if sample_rate != SAMPLE_RATE:
            try:
                import librosa
                logger.info(f"Reamostrando áudio de {sample_rate}Hz para {SAMPLE_RATE}Hz")
                audio_array = librosa.resample(
                    audio_array.astype(np.float32), 
                    orig_sr=sample_rate, 
                    target_sr=SAMPLE_RATE
                )
            except ImportError:
                logger.warning("Librosa não disponível para reamostragem, usando áudio original")
                # Tenta uma reamostragem simples (menos precisa)
                ratio = SAMPLE_RATE / sample_rate
                if ratio > 1:  # Upsampling
                    # Repetir amostras (aproximação simples)
                    audio_array = np.repeat(audio_array, int(ratio))
                elif ratio < 1:  # Downsampling
                    # Pular amostras (aproximação simples)
                    audio_array = audio_array[::int(1/ratio)]
            except Exception as e:
                logger.error(f"Erro ao fazer resampling: {e}")
                
        # Verifica se o áudio não é silêncio completo
        if np.abs(audio_array).max() > 0:
            # Normalização de amplitude para usar o intervalo completo
            normalized = audio_array / np.abs(audio_array).max()
            
            # Ajuste de volume para evitar volumes muito baixos
            rms = np.sqrt(np.mean(normalized**2))
            if rms < 0.05:  # Se o volume RMS for muito baixo
                gain_factor = 0.05 / rms if rms > 0 else 1.0
                normalized = np.clip(normalized * gain_factor, -1.0, 1.0)
                
            return normalized
        return audio_array
    
    def _optimize_options(self, audio_duration: float, is_segment: bool = False) -> Dict[str, Any]:
        """
        Otimiza as opções de transcrição com base na duração do áudio.
        
        Para segmentos mais curtos, podemos usar configurações mais leves.
        Para segmentos mais longos, precisamos de mais qualidade.
        
        Parâmetros:
            audio_duration (float): Duração do áudio em segundos
            is_segment (bool): Se estamos processando um segmento do stream
            
        Retorna:
            Dict[str, Any]: Dicionário com as opções otimizadas
        """
        # Configurações básicas para todos os casos
        options = {
            "language": self.language,
            "fp16": self.device == "cuda",  # Use fp16 apenas em GPU
            "temperature": 0.0,  # Deterministic output
            "condition_on_previous_text": True,  # Melhor para context
            "compression_ratio_threshold": 2.4,  # Valor ajustado para menos alucinações
        }
        
        # Para streams em tempo real, otimiza para velocidade
        if is_segment:
            # Quanto menor o áudio, menos recursos computacionais
            if audio_duration < 3.0:
                options.update({
                    "beam_size": 1,  # Sem beam search
                    "best_of": 1,    # Sem candidatos múltiplos
                })
            elif audio_duration < 10.0:
                options.update({
                    "beam_size": 2,  # Beam search pequeno
                    "best_of": 2     # Poucos candidatos
                })
            else:
                # Para áudio mais longo, podemos usar configurações melhores
                options.update({
                    "beam_size": 3,
                    "best_of": 3
                })
                
        # Para arquivos completos (não streaming), usa configurações melhores
        else:
            # Prioriza qualidade sobre velocidade para arquivos longos
            if audio_duration > 60.0:  # Mais de 1 minuto
                options.update({
                    "beam_size": 5,
                    "best_of": 5
                })
            elif audio_duration > 20.0:  # Entre 20s e 1 minuto
                options.update({
                    "beam_size": 4,
                    "best_of": 4
                })
            else:  # Para arquivos curtos
                options.update({
                    "beam_size": 3,
                    "best_of": 3
                })
        
        # Em CUDA, sempre podemos melhorar um pouco a qualidade
        if self.device == "cuda":
            # Se temos CUDA, aumentamos ligeiramente a qualidade
            if options["beam_size"] < 5:
                options["beam_size"] += 1
                options["best_of"] += 1

        return options

    def _ensure_mono_audio(self, audio: np.ndarray, name: str = "audio") -> np.ndarray:
        """
        Garante que o áudio esteja no formato mono (um único canal).
        
        Parâmetros:
            audio (np.ndarray): Array de áudio a ser verificado
            name (str): Nome para identificação em logs
            
        Retorna:
            np.ndarray: Array de áudio garantidamente mono
        """
        if audio is None:
            return None
            
        # Verifica se é um array numpy
        if not isinstance(audio, np.ndarray):
            logger.warning(f"{name} não é um numpy array, convertendo")
            try:
                audio = np.array(audio, dtype=np.float32)
            except:
                logger.error(f"Não foi possível converter {name} para numpy array")
                return np.zeros(0, dtype=np.float32)
        
        # Log para diagnóstico
        logger.info(f"{name} shape: {audio.shape}, dtype: {audio.dtype}")
        
        # Se for multi-canal, converte para mono
        if len(audio.shape) > 1:
            if audio.shape[1] > 1:  # Se tiver múltiplos canais
                logger.info(f"Convertendo {name} de multi-canal para mono")
                audio = np.mean(audio, axis=1)
            else:  # Se for 2D mas com apenas um canal
                audio = audio.flatten()
        
        return audio

    def transcribe_stream(self, audio_data: np.ndarray, accumulate: bool = True) -> Tuple[np.ndarray, str]:
        """
        Transcreve dados de áudio em tempo real (streaming).
        
        Esta função é otimizada para processar pequenos pedaços de áudio
        à medida que são capturados, mantendo contexto entre os segmentos.
        
        Parâmetros:
            audio_data (np.ndarray): Array numpy com dados de áudio
            accumulate (bool): Se deve acumular contexto entre chamadas
            
        Retorna:
            Tuple[np.ndarray, str]: Tupla com o áudio processado e a transcrição
        """
        # Inicializa buffer e histórico se não existem
        if not hasattr(self, 'stream_buffer'):
            self.stream_buffer = np.array([], dtype=np.float32)
            self._transcription_history = []
            self.accumulated_text = ""
            self._last_processed = time.time()
            self._last_segment_ended_with_pause = False
            self._pause_punctuation = ". "  # Pontuação para adicionar em pausas
            
        # Adiciona o novo áudio ao buffer
        if audio_data is not None and len(audio_data) > 0:
            # Normaliza se necessário
            if audio_data.dtype != np.float32:
                audio_data = audio_data.astype(np.float32) / 32768.0
                
            # Anexa ao buffer existente
            self.stream_buffer = np.concatenate([self.stream_buffer, audio_data])
        
        # Define limites de processamento
        min_interval = 0.3  # segundos entre processamentos (reduzido para maior responsividade)
        current_time = time.time()
        
        # Processa apenas se tiver dados suficientes e tempo mínimo passado
        if len(self.stream_buffer) > SAMPLE_RATE * 0.5 and (current_time - self._last_processed) >= min_interval:
            self._last_processed = current_time
            
            # Detecta se o segmento de áudio termina com silêncio (pausa natural)
            # Usa os últimos 500ms para determinar
            silence_threshold = 0.005
            silence_window = int(0.5 * SAMPLE_RATE)  # 500ms de janela para verificar silêncio
            
            is_ending_with_silence = False
            if len(self.stream_buffer) > silence_window:
                end_segment = self.stream_buffer[-silence_window:]
                rms = np.sqrt(np.mean(np.square(end_segment)))
                is_ending_with_silence = rms < silence_threshold
            
            # Processa o buffer
            buffer, transcription = self._process_stream_buffer(self.stream_buffer)
            
            # Converte o texto para melhor representar pausas naturais
            if transcription:
                # Se temos histórico e o segmento anterior terminou com pausa
                if hasattr(self, '_last_segment_ended_with_pause') and self._last_segment_ended_with_pause:
                    # Verifica se devemos adicionar pontuação para pausas longas
                    if not transcription.startswith(".") and not transcription.startswith(","):
                        # Capitaliza a primeira letra após uma pausa
                        if len(transcription) > 0:
                            transcription = transcription[0].upper() + transcription[1:]
                
                # Verifica se este segmento termina com uma pausa natural
                self._last_segment_ended_with_pause = is_ending_with_silence
                
                # Se termina com silêncio, podemos adicionar pontuação apropriada
                if is_ending_with_silence and not transcription.endswith(".") and not transcription.endswith(","):
                    # Evita adicionar pontuação no meio de palavras
                    if not transcription.endswith(" "):
                        transcription += self._pause_punctuation
            
            # Atualiza o buffer (mantém apenas os últimos 2 segundos para sobreposição)
            overlap = 2.0 * SAMPLE_RATE
            if len(buffer) > overlap:
                self.stream_buffer = buffer[-int(overlap):]
            else:
                self.stream_buffer = buffer
                
            return buffer, transcription
        
        return self.stream_buffer, ""

    def clear_stream_context(self):
        """
        Limpa o contexto do stream, incluindo buffer e histórico.
        Útil para reiniciar a transcrição ou entre frases distintas.
        """
        if hasattr(self, 'stream_buffer'):
            self.stream_buffer = np.array([], dtype=np.float32)
            
        if hasattr(self, '_transcription_history'):
            self._transcription_history = []
            
        if hasattr(self, 'accumulated_text'):
            self.accumulated_text = ""
            
        self._last_processed = time.time()
        
        # Limpa recursos de GPU se disponível
        if torch.cuda.is_available() and self.device == "cuda":
            torch.cuda.empty_cache()
    
    def transcribe_file(self, file_path: str, initial_prompt: str = None) -> str:
        """
        Transcreve um arquivo de áudio WAV usando o modelo Whisper.
        
        Parâmetros:
            file_path (str): Caminho para o arquivo WAV a ser transcrito
            initial_prompt (str, opcional): Texto inicial para dar contexto
            
        Retorna:
            str: Texto transcrito do áudio
        """
        logger.info(f"Transcrevendo arquivo: {file_path}")
        
        # Verificar se o arquivo existe
        if not os.path.exists(file_path):
            logger.error(f"Arquivo não encontrado: {file_path}")
            return ""
            
        try:
            # Carrega o áudio usando a função nativa do Whisper que cuida da normalização
            start_load = time.time()
            audio_array = whisper.load_audio(file_path)
            logger.info(f"Áudio carregado em {time.time() - start_load:.2f}s")
            
            # Normaliza o áudio para melhorar a qualidade da transcrição
            audio_array = self._normalize_audio(audio_array)
            
            # Calcula a duração do áudio em segundos
            audio_duration = len(audio_array) / 16000  # Whisper usa 16kHz
            
            # Obtém opções otimizadas com base na duração do áudio
            options = self._optimize_options(audio_duration)
            
            # Cria um prompt melhorado
            enhanced_prompt = initial_prompt
            
            # Se temos histórico de transcrições e um prompt inicial, enriquece o contexto
            if initial_prompt and len(self._transcription_history) > 0:
                # Limita o tamanho do histórico para evitar contexto muito extenso
                recent_history = self._transcription_history[-3:]
                # Combina o histórico com o prompt inicial
                history_text = ' '.join(recent_history)
                
                # Usa o início do texto histórico seguido pelo prompt mais recente
                # para dar contexto sem sobrecarregar o modelo
                if len(history_text) > 200:
                    history_text = history_text[:200] + "..."
                    
                enhanced_prompt = f"{history_text} {initial_prompt}"
                logger.info(f"Prompt enriquecido criado com {len(enhanced_prompt)} caracteres")
            
            # Adiciona o prompt ao dicionário de opções
            if enhanced_prompt:
                options["initial_prompt"] = enhanced_prompt
            
            # Inicia a contagem de tempo para a transcrição
            start_time = time.time()
            
            # Transcreve o áudio com o modelo Whisper
            result = self.model.transcribe(audio_array, **options)
            
            # Obtém o texto da transcrição
            text = result.get("text", "").strip()
            
            # Registra o tempo de processamento
            processing_time = time.time() - start_time
            logger.info(f"Transcrição concluída em {processing_time:.2f}s. Obtidos {len(text)} caracteres.")
            
            # Armazena este resultado no histórico para uso futuro
            if text:
                self._transcription_history.append(text)
                # Limita o tamanho do histórico para evitar uso excessivo de memória
                if len(self._transcription_history) > 10:
                    self._transcription_history = self._transcription_history[-10:]
            
            return text
            
        except Exception as e:
            logger.error(f"Erro ao transcrever arquivo {file_path}: {e}")
            return f"[ERRO DE TRANSCRIÇÃO: {str(e)}]"

    def transcribe(self, audio: np.ndarray, sample_rate: int = SAMPLE_RATE) -> str:
        """
        Transcreve áudio usando o modelo Whisper.
        
        Args:
            audio: Array numpy de dados de áudio
            sample_rate: Taxa de amostragem do áudio
            
        Returns:
            Texto transcrito
        """
        try:
            # Garantir que o áudio é um array numpy do tipo float32
            if not isinstance(audio, np.ndarray):
                logger.warning(f"Entrada não é numpy array, convertendo")
                audio = np.array(audio, dtype=np.float32)
            elif audio.dtype != np.float32:
                logger.info(f"Convertendo áudio de {audio.dtype} para float32")
                audio = audio.astype(np.float32)
            
            # Garantir que o áudio é mono (1D)
            audio = self._ensure_mono_audio(audio)
            
            # Normaliza o áudio para o formato esperado pelo modelo (inclui resampling se necessário)
            audio_norm = self._normalize_audio(audio, sample_rate)
        
            # Obtém a duração do áudio em segundos
            duration = len(audio) / sample_rate
            logger.info(f"Transcrevendo áudio de {duration:.2f} segundos")
            
            # Otimiza as opções com base na duração
            options = self._get_options_for_duration(duration)
            
            # Executa a transcrição
            start_time = time.time()
            result = self.model.transcribe(
                audio_norm, 
                **options
            )
            
            transcription_time = time.time() - start_time
            logger.info(f"Transcrição concluída em {transcription_time:.2f} segundos")
            
            # Retorna o texto transcrito
            return result["text"].strip()
        except Exception as e:
            logger.error(f"Erro na transcrição: {e}")
            return f"[ERRO: {str(e)}]"
        
    def _get_options_for_duration(self, duration: float) -> Dict:
        """
        Determina as melhores opções de transcrição com base na duração do áudio.
        
        Args:
            duration: Duração do áudio em segundos
            
        Returns:
            Dicionário com as opções de transcrição
        """
        # Opções básicas
        options = {
            "language": self.language,
            "task": "translate" if self.translate else "transcribe",
            "temperature": TEMPERATURE
        }
        
        # Para áudios muito curtos, não use VAD (Voice Activity Detection)
        if duration < 5.0:
            # Configurações para áudio curto, sem segmentação
            options["temperature"] = 0  # Para determinismo máximo
        # Para áudios de comprimento médio, use configurações padrão
        elif duration < 30.0:
            # Mantém configurações padrão
            pass
        # Para áudios longos, use segmentação para processamento em partes
        else:
            # Usa segmentação para áudios longos
            options["segment_length"] = SEGMENT_LENGTH  # Tamanho do segmento em segundos
            
        return options
        
    def set_language(self, language_code: str):
        """Define o idioma para transcrição."""
        self.language = language_code
        logger.info(f"Idioma definido: {language_code}")
        
    def set_translation(self, translate: bool):
        """Define se deve traduzir para inglês."""
        self.translate = translate
        logger.info(f"Tradução: {'ativada' if translate else 'desativada'}")
        
    def start_stream(self, callback_fn=None):
        """
        Inicia o processamento de áudio em streaming.
        
        Args:
            callback_fn: Função que será chamada com o texto transcrito
                         Assinatura: callback_fn(texto, final)
        """
        if self.streaming:
            logger.warning("Streaming já está em execução")
            return
        
        self.streaming = True
        self.stream_callback = callback_fn
        self.stream_queue = queue.Queue()
        self.stream_buffer = []
        self.accumulated_text = ""
        
        # Inicia thread para processar o streaming
        self.stream_thread = threading.Thread(target=self._stream_processor, daemon=True)
        self.stream_thread.start()
        
        logger.info("Streaming de transcrição iniciado")
    
    def stop_stream(self):
        """Para o processamento de streaming."""
        if not self.streaming:
            return
            
        self.streaming = False
        
        # Adiciona None na fila para sinalizar o fim
        if self.stream_queue:
            self.stream_queue.put(None)
            
        # Aguarda a thread terminar
        if self.stream_thread and self.stream_thread.is_alive():
            self.stream_thread.join(timeout=2.0)
            
        logger.info("Streaming de transcrição finalizado")
        
    def _stream_processor(self):
        """Thread que processa o streaming de áudio."""
        try:
            while self.streaming:
                # Espera por novos dados na fila
                item = self.stream_queue.get(timeout=1.0)
                
                # None indica fim do streaming
                if item is None:
                    # Processa o buffer final
                    if self.stream_buffer:
                        self._process_stream_buffer(is_final=True)
                    break
                    
                # Adiciona o chunk ao buffer
                frames, is_final = item
                self.stream_buffer.append(frames)
                
                # Processa o buffer se tiver dados suficientes ou for final
                if len(self.stream_buffer) >= 5 or is_final:
                    self._process_stream_buffer(is_final)
                    
                self.stream_queue.task_done()
                
        except queue.Empty:
            pass
        except Exception as e:
            logger.error(f"Erro no processamento de streaming: {e}")
        finally:
            self.streaming = False
            logger.info("Processador de streaming encerrado")
            
    def _process_stream_buffer(self, is_final=False):
        """
        Processa o buffer de streaming acumulado.
        
        Args:
            is_final: Se True, este é o último processamento
        """
        if not self.stream_buffer:
            return
            
        # Concatena todos os frames
        all_audio = np.concatenate(self.stream_buffer)
        self.stream_buffer = []
        
        # Normaliza o áudio
        audio_norm = self._normalize_audio(all_audio)
        
        # Reduz o limite mínimo para processar áudio curto
        # Permite processar qualquer áudio quando for o último chunk
        if len(audio_norm) < 4000 and not is_final:  # menos de 0.25 segundos a 16kHz
            return
            
        try:
            # Executa a transcrição
            options = {
                "language": self.language,
                "task": "translate" if self.translate else "transcribe",
                "temperature": 0  # Para determinismo no streaming
            }
            
            # Usa prompt inicial se tivermos texto acumulado
            if self.accumulated_text:
                options["initial_prompt"] = self.accumulated_text
                
            # Forçar opções mais favoráveis para trechos curtos
            if len(audio_norm) < 16000:  # menos de 1 segundo
                options["no_speech_threshold"] = 0.3  # Mais sensível
            
            result = self.model.transcribe(audio_norm, **options)
            
            # Obtém o texto
            text = result["text"].strip()
            
            # Se tivermos texto acumulado, evita repetições
            if self.accumulated_text and text and text.lower().startswith(self.accumulated_text.lower()):
                # Remove sobreposição
                new_text = text[len(self.accumulated_text):].strip()
                if new_text:
                    # Separa com espaço se necessário
                    if not self.accumulated_text.endswith(" ") and not new_text.startswith(" "):
                        new_text = " " + new_text
                    self.accumulated_text += new_text
            elif text:
                # Se não houver sobreposição, mas tivermos um novo texto
                if self.accumulated_text:
                    # Adiciona espaço entre textos
                    if not self.accumulated_text.endswith(" ") and not text.startswith(" "):
                        self.accumulated_text += " " + text
                    else:
                        self.accumulated_text += text
                else:
                    # Se não temos texto acumulado, inicia com este
                    self.accumulated_text = text
            
            # Chama o callback com o texto acumulado
            if self.stream_callback:
                self.stream_callback(self.accumulated_text, is_final)
                
        except Exception as e:
            logger.error(f"Erro ao transcrever stream: {e}")
            # Mesmo em caso de erro, tenta chamar o callback com o texto acumulado
            if self.stream_callback and self.accumulated_text:
                self.stream_callback(self.accumulated_text, is_final)
    
    def process_stream(self, frames, is_final):
        """
        Processa frames de áudio para transcrição em tempo real.
        Este método é usado como um processador de chunks para o AudioRecorder.
        
        Args:
            frames: Lista de arrays numpy com dados de áudio
            is_final: True se for o último chunk do stream
        """
        if not self.streaming:
            return
        
        try:
            # Adiciona os frames à fila de processamento
            # Concatena todos os frames em um único array
            if frames:
                all_frames = np.concatenate(frames)
                self.stream_queue.put((all_frames, is_final))
                
            # Se for o último frame, adiciona um None para indicar o fim
            if is_final:
                self.stream_queue.put(None)
                
        except Exception as e:
            logger.error(f"Erro ao enviar frames para o stream: {e}")

    def _process_stream_buffer(self, audio_buffer: np.ndarray) -> Tuple[np.ndarray, str]:
        """
        Processa o buffer de stream de áudio para transcrição.
        
        Esta função é utilizada internamente pelo módulo de streaming.
        Otimiza o processamento do buffer baseado em seu tamanho.
        
        Parâmetros:
            audio_buffer (np.ndarray): Buffer de áudio a ser processado
            
        Retorna:
            Tuple[np.ndarray, str]: Tupla com o áudio processado e a transcrição
        """
        # Verifica se o buffer tem tamanho suficiente para processamento
        min_duration = 1.0  # segundos
        min_samples = int(min_duration * SAMPLE_RATE)
        
        if audio_buffer is None or len(audio_buffer) < min_samples:
            return audio_buffer, ""
        
        # Garante tipo de dados adequado e normaliza se necessário
        if audio_buffer.dtype != np.float32:
            audio_buffer = audio_buffer.astype(np.float32) / 32768.0
        
        # Aumenta o tamanho máximo do buffer para mais contexto
        max_duration = 45.0  # segundos máximos (aumentado de 30s)
        max_samples = int(max_duration * SAMPLE_RATE)
        
        if len(audio_buffer) > max_samples:
            # Preserva apenas os dados mais recentes
            audio_buffer = audio_buffer[-max_samples:]
        
        # Garante que o áudio é contíguo na memória para processamento eficiente
        audio_buffer = np.ascontiguousarray(audio_buffer)
        
        try:
            # Usa configurações otimizadas para streaming
            duration = len(audio_buffer) / SAMPLE_RATE
            options = self._optimize_options(duration, is_segment=True)
            
            # Se temos texto acumulado, usamos como contexto
            if hasattr(self, 'accumulated_text') and self.accumulated_text:
                # Aumenta o limite de contexto para melhorar a continuidade
                context_limit = 500 if duration > 5.0 else 250
                options["initial_prompt"] = self.accumulated_text[-context_limit:]
                
                # Adiciona o último segmento como prompt para manter a continuidade
                if hasattr(self, '_last_segment') and self._last_segment:
                    options["initial_prompt"] = f"{options['initial_prompt']} {self._last_segment}"
            
            # Recolhe lixo antes de processamento pesado
            gc.collect()
            if torch.cuda.is_available() and self.device == "cuda":
                torch.cuda.empty_cache()
            
            # Processa o áudio
            start_time = time.time()
            result = self.model.transcribe(audio_buffer, **options)
            processing_time = time.time() - start_time
            
            # Log de desempenho para ajuste
            logger.debug(f"Buffer de {duration:.1f}s processado em {processing_time:.2f}s (ratio: {processing_time/duration:.2f}x)")
            
            # Extrai o texto e atualiza histórico
            text = result.get("text", "").strip()
            
            if text:
                # Inicializa histórico se não existir
                if not hasattr(self, '_transcription_history'):
                    self._transcription_history = []
                    
                # Armazena o último segmento completo para próxima referência
                self._last_segment = text
                
                # Atualiza histórico para context futuro
                if len(self._transcription_history) >= 10:  # Aumentado de 5
                    self._transcription_history.pop(0)  # Remove o mais antigo
                
                self._transcription_history.append(text)
                self.accumulated_text = " ".join(self._transcription_history)
            
            # Libera memória
            del result
            if duration > 10.0:
                gc.collect()
                if torch.cuda.is_available() and self.device == "cuda":
                    torch.cuda.empty_cache()
            
            return audio_buffer, text
            
        except Exception as e:
            logger.error(f"Erro no processamento do buffer: {str(e)}")
            return audio_buffer, f"[Erro: {str(e)}]"

# Função auxiliar para checar disponibilidade de GPU
def check_gpu():
    """Verifica se GPU está disponível e retorna informações sobre ela."""
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        devices = [
            f"Device {i}: {torch.cuda.get_device_name(i)}" 
            for i in range(device_count)
        ]
        return {
            "available": True,
            "device_count": device_count,
            "devices": devices,
            "current_device": torch.cuda.current_device()
        }
    else:
        return {"available": False} 